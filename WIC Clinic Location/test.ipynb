{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Wayback Machine \n",
    "\n",
    "## Get snapshots of the URL we want to scrape\n",
    "\n",
    " For all URL in the Wisconsin Health Department's website page with al WIC local offices, scrape the Wayback Machine for all versions of the URL.\n",
    "\n",
    " **Note** This run in console for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-02 21:23:35 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\n",
      "2022-02-02 21:23:35 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.7 (default, Sep 16 2021, 13:09:58) - [GCC 7.5.0], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Linux-5.13.0-28-generic-x86_64-with-glibc2.31\n",
      "2022-02-02 21:23:35 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'AUTOTHROTTLE_START_DELAY': 1,\n",
      " 'AUTOTHROTTLE_TARGET_CONCURRENCY': 10.0,\n",
      " 'LOG_LEVEL': 'INFO',\n",
      " 'USER_AGENT': 'Wayback Machine Scraper/1.0.8 '\n",
      "               '(+https://github.com/sangaline/scrapy-wayback-machine)'}\n",
      "2022-02-02 21:23:35 [scrapy.extensions.telnet] INFO: Telnet Password: e850e8c825195d19\n",
      "2022-02-02 21:23:35 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n",
      "2022-02-02 21:23:35 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy_wayback_machine.WaybackMachineMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-02-02 21:23:35 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-02-02 21:23:35 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-02-02 21:23:35 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-02-02 21:23:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-02-02 21:23:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-02-02 21:23:43 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-02-02 21:23:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 12056,\n",
      " 'downloader/request_count': 35,\n",
      " 'downloader/request_method_count/GET': 35,\n",
      " 'downloader/response_bytes': 996320,\n",
      " 'downloader/response_count': 35,\n",
      " 'downloader/response_status_count/200': 34,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'dupefilter/filtered': 12,\n",
      " 'elapsed_time_seconds': 7.44137,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 2, 3, 3, 23, 43, 48595),\n",
      " 'httpcompression/response_bytes': 4447822,\n",
      " 'httpcompression/response_count': 34,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 62115840,\n",
      " 'memusage/startup': 62115840,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 33,\n",
      " 'scheduler/dequeued': 37,\n",
      " 'scheduler/dequeued/memory': 37,\n",
      " 'scheduler/enqueued': 37,\n",
      " 'scheduler/enqueued/memory': 37,\n",
      " 'start_time': datetime.datetime(2022, 2, 3, 3, 23, 35, 607225)}\n",
      "2022-02-02 21:23:43 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "! wayback-machine-scraper -a 'https://www.dhs.wisconsin.gov/wic/local-projects.htm$' https://www.dhs.wisconsin.gov/wic/local-projects.htm -o './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates the structure diretories in `./data/`:\n",
    "\n",
    "```\n",
    "website/\n",
    "└── www.dhs.wisconsin.gov\n",
    "    └── wic\n",
    "        └──local-projects.htm\n",
    "            ├── 20141213083147.snapshot\n",
    "            ├── 20150801041055.snapshot\n",
    "            ...\n",
    "            └── 20211211141555.snapshot\n",
    "```\n",
    "\n",
    "We can obtain a list of retrieved snapshots by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "data_path = './data/www.dhs.wisconsin.gov/wic/local-projects.htm/'\n",
    "\n",
    "snapshots = [f for f in listdir(data_path) if isfile(join(data_path, f)) and f.endswith('.snapshot')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data from the snapshots\n",
    "\n",
    "### Example: Scrape data from the oldest snapshot\n",
    "\n",
    "Since this page structure is simple enough we can extract the data using just pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "with open(data_path + snapshots[0]) as fp:\n",
    "    soup = BeautifulSoup(fp, 'html.parser')\n",
    "\n",
    "table = soup.find_all('table')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rows = table.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_county  = \n",
    "for i in range(1, len(table_rows)):\n",
    "    county = table_rows[i].find_all('td')[0].text.strip()\n",
    "    content = table_rows[i].find_all('td')[1].find_all('p')\n",
    "\n",
    "    data_county = { \"county\":[], \"zip\": [], \"address\" : []}#, \"name\" : []}\n",
    "\n",
    "    for c in content:\n",
    "        t = c.text.strip().split('\\n\\t\\t\\t\\t\\t\\t')\n",
    "        # if (len(t) == 1) and ('Back to top ' not in t[0]):\n",
    "            # data_county['name'].append(t[0])\n",
    "        # el\n",
    "        if (len(t) > 1):\n",
    "            data_county['county'].append(county)\n",
    "            t = \" \".join(t).split('Telephone:')[0]\n",
    "            data_county['address'].append(t)\n",
    "            data_county['zip'].append(re.findall(r\"(?<!\\d)\\d{5}(?!\\d)\", t)[0])\n",
    "    try:\n",
    "        pd.DataFrame(data_county)\n",
    "    except:\n",
    "        print(i)\n",
    "        display(data_county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 41\n",
    "county = table_rows[i].find_all('td')[0].text.strip()\n",
    "content = table_rows[i].find_all('td')[1].find_all('p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7120 West National Ave. West Allis, WI 53214 \n",
      "5050 S. Lake Drive Cudahy, WI 53116 \n",
      "7325 West Forest Home Ave. Greenfield, WI 53220 \n",
      "1218 West Kilbourn Ave, Suite 207 Milwaukee, WI 53233 \n",
      "1337 South Cesar Chavez Drive Milwaukee, WI 53204 \n",
      "1445 South 32nd St. Milwaukee, WI 53215 \n",
      "Northwest Health Center 7630 West Mill Road Milwaukee, WI 53218 \n",
      "Keenan WIC Project 3200 N 36th St. Milwaukee, WI 53216 \n",
      "South Side Health Center (SSHC) 1639 S. 23rd St., First Floor Milwaukee, WI 53204 \n",
      "2555 North Dr. Martin Luther King Jr. Drive Milwaukee, WI 53212 \n",
      "3882 North Teutonia Ave. Milwaukee, WI 53206 \n",
      "5825 West Capitol Drive Milwaukee, WI 53216 \n",
      "4630 W. North Ave. Milwaukee, WI 53208 \n"
     ]
    }
   ],
   "source": [
    "for c in content:\n",
    "    t = c.text.strip().split('\\n\\t\\t\\t\\t\\t\\t')\n",
    "    if (len(t) == 1) and ('Back to top ' not in t[0]):\n",
    "            data_county['name'].append(t[0])\n",
    "            data_county['county'].append(county)\n",
    "    elif (len(t) > 1):\n",
    "        t = \" \".join(t).split('Telephone:')[0]\n",
    "        print(t)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25ba7b2fce11290ff058c342a2e5c4e405c72fe334204cebd2d9f7aaf9db2ad5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
